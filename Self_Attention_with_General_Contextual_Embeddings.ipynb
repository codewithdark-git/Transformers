{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7RhSMz9rJRg8Pb0IbnY9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codewithdark-git/Transformers/blob/main/Self_Attention_with_General_Contextual_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txU5uX-WJFpk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction to Self-Attention with General Contextual Embeddings**\n",
        "\n",
        "```\n",
        "Find Code Here >>> github.com/XCollab/HuggingFace\n",
        "```\n",
        "```\n",
        "Follow on GitHub >>> github.com/codewithdark-git\n",
        "```\n",
        "```\n",
        "Connect on Linkedin >>> linkedin.com/in/codewithdark\n",
        "```\n",
        "```\n",
        "Join The OpenSource GitHub Community >>> github.com/XCollab\n",
        "```\n",
        "\n",
        "\n",
        "This notebook delves into the fundamental concept of **self-attention**, a key mechanism behind modern natural language processing (NLP) models like Transformers. Self-attention enables models to capture the relationships between words in a sequence, irrespective of their distance, making it a cornerstone of contextual word embeddings.\n",
        "\n",
        "### **Key Concepts Covered in This Notebook**\n",
        "\n",
        "1. **General Contextual Embeddings**:\n",
        "   - Contextual embeddings represent words based on their meaning in a specific context, as opposed to static embeddings like Word2Vec or GloVe. These embeddings dynamically adjust based on surrounding words, capturing semantic nuances.\n",
        "   - In this notebook, we begin by creating a co-occurrence-based embedding space to represent words and their relationships.\n",
        "\n",
        "2. **Dot Product Similarity for Attention Scores**:\n",
        "   - The **dot product similarity** is used to compute attention scores between words in the embedding space.\n",
        "   - These scores reflect how much \"attention\" one word should pay to another, forming the foundation for self-attention computations.\n",
        "\n",
        "3. **Softmax Transformation**:\n",
        "   - To normalize attention scores into probabilities, the **softmax function** is applied. This ensures the scores are interpretable as weights, summing up to 1 for each word's attention distribution.\n",
        "\n",
        "4. **Matrix Transformation for Contextualization**:\n",
        "   - Using the attention weights, embeddings are transformed to encode contextual information. This highlights how the self-attention mechanism allows words to dynamically incorporate meaning based on their surroundings.\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose and Applications**\n",
        "\n",
        "This notebook aims to provide an intuitive understanding of:\n",
        "- How self-attention computes relationships between words using similarity measures.\n",
        "- The role of softmax normalization in translating similarity scores into probabilistic attention distributions.\n",
        "- The process of generating **contextual embeddings** that capture nuanced word meanings based on their context.\n",
        "\n",
        "By the end of this notebook, you will have a solid grasp of self-attention, its mathematical foundation, and its role in generating contextual embeddings, paving the way for deeper exploration of Transformer-based architectures like BERT, GPT, and beyond.\n"
      ],
      "metadata": {
        "id": "VLTAPOsZOdfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word embeddings**:\n",
        "\n",
        "### **1. Preprocessing: Create Vocabulary**\n",
        "- **Input:** A list of sentences (`corpus`).\n",
        "- **Goal:** Tokenize the sentences into words, and create a vocabulary (`vocab`) that maps each unique word to a unique index.\n",
        "\n",
        "**Explanation:**\n",
        "- Each word is extracted from the sentences.\n",
        "- A dictionary is created where each word is assigned an index.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "If the corpus is `[\"I like AI\", \"AI is amazing\"]`:\n",
        "$$\n",
        "\\text{Vocabulary} = \\{ \\text{\"I\"}: 0, \\text{\"like\"}: 1, \\text{\"AI\"}: 2, \\text{\"is\"}: 3, \\text{\"amazing\"}: 4 \\}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Co-occurrence Matrix**\n",
        "- **Goal:** Create a matrix where each cell represents how often two words co-occur within a context window.\n",
        "\n",
        "**Explanation:**\n",
        "- For each word in the sentence, consider other words within a fixed **window size** around it (e.g., Â±3 words).\n",
        "- Update the co-occurrence count for the word and its neighbors.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "Let:\n",
        "- $( C[i][j] )$: Co-occurrence of word $(i)$ with word $(j)$.\n",
        "- $( \\text{Window Size} = 3 )$.\n",
        "\n",
        "For the sentence \"I like AI is amazing\":\n",
        "$$\n",
        "C[\\text{\"like\"}][\\text{\"I\"}] += 1, \\, C[\\text{\"like\"}][\\text{\"AI\"}] += 1, \\, C[\\text{\"like\"}][\\text{\"is\"}] += 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Normalize the Co-occurrence Matrix**\n",
        "- **Goal:** Normalize each row of the matrix to ensure the sum of each row equals 1.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "Let $( \\text{RowSum}[i] = \\sum_{j=1}^{\\text{vocab\\_size}} C[i][j] )$. The normalized matrix is:\n",
        "$$\n",
        "\\text{Normalized\\_Matrix}[i][j] = \\frac{C[i][j]}{\\text{RowSum}[i]}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Dimensionality Reduction using SVD**\n",
        "- **Goal:** Reduce the dimensions of the co-occurrence matrix to extract meaningful word embeddings.\n",
        "\n",
        "**Explanation:**\n",
        "- Perform **Singular Value Decomposition (SVD)**:\n",
        "$$\n",
        "M = U \\cdot S \\cdot V^T\n",
        "$$\n",
        "Where:\n",
        "  - $( M )$: Co-occurrence matrix.\n",
        "  - $( U )$: Left singular vectors (used for word embeddings).\n",
        "  - $( S $): Singular values (diagonal matrix).\n",
        "  - $( V^T )$: Right singular vectors.\n",
        "\n",
        "- Select the top `embedding_dim` dimensions from $(U)$ to get word embeddings.\n",
        "\n",
        "**Word Embeddings:**\n",
        "Each word is represented as a vector of size `embedding_dim` (e.g., 2).\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Walkthrough**\n",
        "\n",
        "#### 1. Preprocessing:\n",
        "```python\n",
        "words = set(word for sentence in corpus for word in sentence.split())\n",
        "vocab = {word: idx for idx, word in enumerate(words)}\n",
        "vocab_size = len(vocab)\n",
        "```\n",
        "\n",
        "#### 2. Co-occurrence Matrix:\n",
        "```python\n",
        "co_occurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
        "for sentence in corpus:\n",
        "    tokens = sentence.split()\n",
        "    for i, word in enumerate(tokens):\n",
        "        word_idx = vocab[word]\n",
        "        start = max(i - window_size, 0)\n",
        "        end = min(i + window_size + 1, len(tokens))\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                context_word_idx = vocab[tokens[j]]\n",
        "                co_occurrence_matrix[word_idx][context_word_idx] += 1\n",
        "```\n",
        "\n",
        "#### 3. Normalize:\n",
        "```python\n",
        "co_occurrence_matrix = co_occurrence_matrix / np.sum(co_occurrence_matrix, axis=1, keepdims=True)\n",
        "```\n",
        "\n",
        "#### 4. Dimensionality Reduction:\n",
        "```python\n",
        "U, S, Vt = np.linalg.svd(co_occurrence_matrix)\n",
        "embedding_dim = 2\n",
        "word_embeddings = U[:, :embedding_dim]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Outputs**\n",
        "- Prints:\n",
        "  - Vocabulary.\n",
        "  - Co-occurrence matrix.\n",
        "  - Word embeddings.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## **Dot product similarity matrix**:\n",
        "---\n",
        "\n",
        "### **1. Input: Word Embeddings**\n",
        "- The `embeddings` dictionary contains words as keys and their corresponding embedding vectors as values.\n",
        "- Example:\n",
        "  ```python\n",
        "  \"like\": [-0.23339497, -0.01719972]\n",
        "  \"calm\": [-0.23410982, 0.20496999]\n",
        "  ```\n",
        "  Each word is represented by a 2-dimensional vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Convert Embeddings to Matrix**\n",
        "- The embeddings are converted into a matrix where each row corresponds to the embedding of a word.\n",
        "\n",
        "**Mathematical Representation:**\n",
        "If there are $(n)$ words and each word embedding has $(d)$ dimensions:\n",
        "$$\n",
        "\\text{Embedding Matrix} = \\begin{bmatrix}\n",
        "e_{11} & e_{12} & \\dots & e_{1d} \\\\\n",
        "e_{21} & e_{22} & \\dots & e_{2d} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "e_{n1} & e_{n2} & \\dots & e_{nd}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Where $(e_{ij})$ represents the $(j)-th$ dimension of the embedding for word $(i)$.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Compute the Dot Product Similarity**\n",
        "- The **dot product similarity** between two vectors $(u)$ and $(v)$ is defined as:\n",
        "$$\n",
        "u \\cdot v = \\sum_{i=1}^d u_i v_i\n",
        "$$\n",
        "Where \\(d\\) is the number of dimensions in the embeddings.\n",
        "\n",
        "- By computing the dot product for all pairs of words, we generate a similarity matrix.\n",
        "\n",
        "**Similarity Matrix Formula:**\n",
        "If $(E)$ is the embedding matrix $(n \\times d)$, the similarity matrix\n",
        "$$\n",
        "S = E \\cdot E^T\n",
        "$$\n",
        "Where $(S[i, j])$ is the dot product similarity between the $(i)-th$ and $(j)-th$ words.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Display the Results**\n",
        "The similarity values for each word pair are printed in a matrix-like format. For example:\n",
        "```\n",
        "like Â· like = 0.0548\n",
        "like Â· calm = 0.0343\n",
        "like Â· place = 0.0265\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Output Example**\n",
        "The output will display the similarity of every word with every other word, including itself (the diagonal of the matrix will show self-similarities).\n",
        "\n",
        "For instance:\n",
        "```\n",
        "Dot Product Similarity Matrix:\n",
        "like Â· like = 0.0548\n",
        "like Â· calm = 0.0343\n",
        "like Â· place = 0.0265\n",
        "...\n",
        "\n",
        "calm Â· like = 0.0343\n",
        "calm Â· calm = 0.0983\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **What Does the Dot Product Similarity Represent?**\n",
        "- The dot product measures how aligned two embedding vectors are in the vector space.\n",
        "- A **higher value** indicates greater similarity in terms of meaning or usage in the dataset that generated the embeddings.\n",
        "- A **lower (or negative) value** indicates less similarity or orthogonality between the two vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Insights**\n",
        "1. **Self-similarity:** The dot product of a word with itself represents the magnitude squared of the embedding vector:\n",
        "$$\n",
        "   \\text{Self-similarity for } u: \\, u \\cdot u = ||u||^2\n",
        "$$\n",
        "2. **Symmetry:** The similarity matrix is symmetric:\n",
        "$$\n",
        "   u \\cdot v = v \\cdot u\n",
        "$$\n",
        "\n",
        "---\n",
        "## **Softmax**:\n",
        "---\n",
        "\n",
        "### **1. What is Softmax?**\n",
        "- The **softmax** function transforms a vector (or matrix) of real values into a probability distribution.\n",
        "- The output values are in the range $([0, 1])$, and the rows of the matrix will sum to 1.\n",
        "- It is defined as:\n",
        "$$\n",
        "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n",
        "Where:\n",
        "  - $(x_i)$: The $(i)-th$ element of the input vector.\n",
        "  - $(n)$: The total number of elements in the input vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Function Explanation**\n",
        "\n",
        "#### **Step 1: Exponentiate Each Element**\n",
        "```python\n",
        "exp_matrix = np.exp(matrix)\n",
        "```\n",
        "- The exponential function $(e^x)$ is applied to each element of the input matrix.\n",
        "- This ensures all values are positive and highlights larger values.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Compute Row-Wise Sums**\n",
        "```python\n",
        "row_sums = np.sum(exp_matrix, axis=1, keepdims=True)\n",
        "```\n",
        "- The sum of the exponentials is computed for each row.\n",
        "- `keepdims=True` ensures the result has the same dimensionality as `exp_matrix` for proper broadcasting.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Normalize Rows**\n",
        "```python\n",
        "return exp_matrix / row_sums\n",
        "```\n",
        "- Each element of the matrix is divided by the sum of its row.\n",
        "- This step ensures that the rows sum to 1, making the output a valid probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Weighted Contextual Embeddings**\n",
        "The normalized similarity weights are then used to compute **contextual embeddings**:\n",
        "\n",
        "$$\n",
        "\\text{ContextualEmbedding}(i) = \\sum_{j} \\text{Weight}(i, j) \\cdot \\text{Embedding}(j)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Representation**\n",
        "For an input matrix $(M)$ of size $(m \\times n)$, where $(M_{ij})$ represents the element in the $(i)-th$ row and $(j)-th$ column:\n",
        "1. Compute the exponentials:\n",
        "$$\n",
        "E_{ij} = e^{M_{ij}}\n",
        "$$\n",
        "2. Compute row sums:\n",
        "$$\n",
        "\\text{RowSum}_i = \\sum_{j=1}^n E_{ij}\n",
        "$$\n",
        "3. Normalize:\n",
        "$$\n",
        "\\text{Softmax}_{ij} = \\frac{E_{ij}}{\\text{RowSum}_i}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Softmax?**\n",
        "- **Probability Interpretation:** Converts arbitrary scores into probabilities, where higher scores correspond to higher probabilities.\n",
        "- **Classification:** Often used in the output layer of neural networks for multi-class classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Usage**\n",
        "\n",
        "#### Input:\n",
        "```python\n",
        "matrix = np.array([[2.0, 1.0, 0.1],\n",
        "                   [1.0, 3.0, 2.0]])\n",
        "```\n",
        "\n",
        "#### Softmax Computation:\n",
        "1. Exponentiate:\n",
        "   \\[\n",
        "   e^{[2.0, 1.0, 0.1]} = [7.389, 2.718, 1.105]\n",
        "   \\]\n",
        "   Similarly for the second row.\n",
        "\n",
        "2. Row-wise sums:\n",
        "   \\[\n",
        "   \\text{RowSum}_1 = 7.389 + 2.718 + 1.105 = 11.212\n",
        "   \\]\n",
        "\n",
        "3. Normalize:\n",
        "   \\[\n",
        "   \\text{Softmax}_1 = [7.389 / 11.212, 2.718 / 11.212, 1.105 / 11.212] = [0.659, 0.242, 0.099]\n",
        "   \\]\n",
        "\n",
        "#### Output:\n",
        "```python\n",
        "[[0.65900114, 0.24243297, 0.09856589],\n",
        " [0.09003057, 0.66524096, 0.24472847]]\n",
        "```\n",
        "\n",
        "---\n",
        "## **matrix multiplication**:\n",
        "---\n",
        "\n",
        "### **1. Matrix Multiplication (Dot Product)**\n",
        "The operation:\n",
        "```python\n",
        "result = np.dot(x, similarity_matrix)\n",
        "```\n",
        "- Computes the dot product between matrix $(x)$ and the `similarity_matrix`.\n",
        "- The **result** is a new matrix whose shape depends on the dimensions of\n",
        "$(x)$ and `similarity_matrix`.\n",
        "\n",
        "#### **Matrix Multiplication Rule:**\n",
        "If $(x)$ has shape $(a, b)$ and `similarity_matrix` has shape $(b, c)$, then the resulting matrix will have shape:\n",
        "\\[\n",
        "\\text{Resulting Matrix Shape: } (a, c)\n",
        "\\]\n",
        "For matrix multiplication to work, the number of columns in $(x)$ $(b)$ must equal the number of rows in `similarity_matrix` $(b)$.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Display Shapes**\n",
        "```python\n",
        "print(\"Softmax Matrix Shape:\", x.shape)\n",
        "print(\"Similarity Matrix Shape:\", similarity_matrix.shape)\n",
        "print(\"Resulting Matrix Shape:\", result.shape)\n",
        "```\n",
        "- **Purpose:** Verifies the shapes of $(x)$, `similarity_matrix`, and `result` to confirm that:\n",
        "  - $(x)$ and `similarity_matrix` are compatible for multiplication.\n",
        "  - The resulting matrix has the expected dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Display Resulting Matrix**\n",
        "```python\n",
        "print(\"\\nResulting Matrix:\")\n",
        "print(result)\n",
        "```\n",
        "- **Purpose:** Prints the final matrix after the dot product.\n",
        "- This matrix could represent various quantities, such as weighted probabilities or scores, depending on the context.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Walkthrough**\n",
        "Letâs assume the following:\n",
        "1. **Input Matrices:**\n",
        "   - $(x)$ has shape $(2, 3)$, e.g.:\n",
        "$$\n",
        "     x = \\begin{bmatrix}\n",
        "     0.1 & 0.3 & 0.6 \\\\\n",
        "     0.5 & 0.4 & 0.1\n",
        "     \\end{bmatrix}\n",
        "$$\n",
        "   - `similarity_matrix` has shape $(3, 4)$, e.g.:\n",
        "$$\n",
        "     \\text{similarity_matrix} = \\begin{bmatrix}\n",
        "     1 & 0 & 2 & 1 \\\\\n",
        "     0 & 1 & 1 & 0 \\\\\n",
        "     1 & 0 & 0 & 1\n",
        "     \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. **Matrix Multiplication:**\n",
        "   - Multiply $(x)$ $(2 \\times 3)$ with `similarity_matrix` $(3 \\times 4)$:\n",
        "$$\n",
        "     \\text{Resulting Matrix} = \\begin{bmatrix}\n",
        "     0.8 & 0.3 & 0.6 & 0.7 \\\\\n",
        "     1.6 & 0.4 & 1.4 & 1.1\n",
        "     \\end{bmatrix}\n",
        "$$\n",
        "   - The result has shape $(2, 4)$.\n",
        "\n",
        "3. **Output:**\n",
        "```\n",
        "Softmax Matrix Shape: (2, 3)\n",
        "Similarity Matrix Shape: (3, 4)\n",
        "Resulting Matrix Shape: (2, 4)\n",
        "```\n",
        "Resulting Matrix:\n",
        "[[0.8 0.3 0.6 0.7]\n",
        " [1.6 0.4 1.4 1.1]]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points to Verify:**\n",
        "1. Ensure the shapes are compatible for multiplication: the number of columns in \\(x\\) matches the number of rows in `similarity_matrix`.\n",
        "2. The resulting matrix's shape should have the same number of rows as \\(x\\) and the same number of columns as `similarity_matrix`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5ooGmxYJIGX"
      }
    }
  ]
}